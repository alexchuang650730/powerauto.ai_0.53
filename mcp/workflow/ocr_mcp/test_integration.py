#!/usr/bin/env python3
"""
OCRå·¥ä½œæµMCPé›†æˆæµ‹è¯•

æµ‹è¯•é‡æ„åçš„OCRå·¥ä½œæµæ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

import os
import sys
import json
import asyncio
import logging
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent.parent.parent.parent
sys.path.append(str(project_root))

# å¯¼å…¥OCRå·¥ä½œæµMCP
from mcp.workflow.ocr_workflow_mcp import OCRWorkflowMCP

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class OCRWorkflowIntegrationTest:
    """OCRå·¥ä½œæµé›†æˆæµ‹è¯•ç±»"""
    
    def __init__(self):
        self.test_results = []
        self.mcp = None
    
    async def setup(self):
        """æµ‹è¯•è®¾ç½®"""
        try:
            # åˆå§‹åŒ–OCRå·¥ä½œæµMCP
            config_dir = Path(__file__).parent / "config"
            self.mcp = OCRWorkflowMCP(str(config_dir))
            logger.info("âœ… OCRå·¥ä½œæµMCPåˆå§‹åŒ–æˆåŠŸ")
            return True
        except Exception as e:
            logger.error(f"âŒ OCRå·¥ä½œæµMCPåˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    async def test_basic_functionality(self):
        """æµ‹è¯•åŸºæœ¬åŠŸèƒ½"""
        test_name = "åŸºæœ¬åŠŸèƒ½æµ‹è¯•"
        logger.info(f"ğŸ§ª å¼€å§‹{test_name}")
        
        try:
            # æµ‹è¯•MCPä¿¡æ¯è·å–
            info = self.mcp.get_info()
            assert "name" in info
            assert "version" in info
            logger.info(f"âœ… MCPä¿¡æ¯è·å–æˆåŠŸ: {info['name']} v{info['version']}")
            
            # æµ‹è¯•èƒ½åŠ›è·å–
            capabilities = self.mcp.get_capabilities()
            assert "capabilities" in capabilities
            assert len(capabilities["capabilities"]) > 0
            logger.info(f"âœ… èƒ½åŠ›è·å–æˆåŠŸ: {len(capabilities['capabilities'])}ä¸ªèƒ½åŠ›")
            
            # æµ‹è¯•å¥åº·æ£€æŸ¥
            health = self.mcp.health_check()
            assert "status" in health
            logger.info(f"âœ… å¥åº·æ£€æŸ¥æˆåŠŸ: {health['status']}")
            
            self.test_results.append({
                "test_name": test_name,
                "status": "PASS",
                "details": "æ‰€æœ‰åŸºæœ¬åŠŸèƒ½æ­£å¸¸"
            })
            
        except Exception as e:
            logger.error(f"âŒ {test_name}å¤±è´¥: {e}")
            self.test_results.append({
                "test_name": test_name,
                "status": "FAIL",
                "error": str(e)
            })
    
    async def test_configuration_loading(self):
        """æµ‹è¯•é…ç½®åŠ è½½"""
        test_name = "é…ç½®åŠ è½½æµ‹è¯•"
        logger.info(f"ğŸ§ª å¼€å§‹{test_name}")
        
        try:
            # æµ‹è¯•é…ç½®è·å–
            config = self.mcp.get_config()
            assert "workflow_config" in config
            assert "routing_rules" in config
            assert "quality_settings" in config
            
            # éªŒè¯å…³é”®é…ç½®é¡¹
            workflow_config = config["workflow_config"]
            assert "workflow" in workflow_config
            assert "execution" in workflow_config
            
            routing_rules = config["routing_rules"]
            assert "routing_rules" in routing_rules
            assert "special_rules" in routing_rules
            
            quality_settings = config["quality_settings"]
            assert "quality" in quality_settings
            assert "limits" in quality_settings
            
            logger.info("âœ… æ‰€æœ‰é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ")
            
            self.test_results.append({
                "test_name": test_name,
                "status": "PASS",
                "details": "é…ç½®æ–‡ä»¶åŠ è½½æ­£å¸¸"
            })
            
        except Exception as e:
            logger.error(f"âŒ {test_name}å¤±è´¥: {e}")
            self.test_results.append({
                "test_name": test_name,
                "status": "FAIL",
                "error": str(e)
            })
    
    async def test_workflow_executor(self):
        """æµ‹è¯•å·¥ä½œæµæ‰§è¡Œå™¨"""
        test_name = "å·¥ä½œæµæ‰§è¡Œå™¨æµ‹è¯•"
        logger.info(f"ğŸ§ª å¼€å§‹{test_name}")
        
        try:
            # æ£€æŸ¥æ‰§è¡Œå™¨æ˜¯å¦æ­£ç¡®åˆå§‹åŒ–
            executor = self.mcp.executor
            assert executor is not None
            
            # æ£€æŸ¥å·¥ä½œæµæ­¥éª¤
            assert hasattr(executor, 'workflow_steps')
            assert len(executor.workflow_steps) > 0
            logger.info(f"âœ… å·¥ä½œæµæ­¥éª¤æ•°é‡: {len(executor.workflow_steps)}")
            
            # æ£€æŸ¥OCRç»„ä»¶
            assert hasattr(executor, 'ocr_components')
            ocr_components = executor.ocr_components
            logger.info(f"âœ… OCRç»„ä»¶æ•°é‡: {len(ocr_components)}")
            
            # æ£€æŸ¥é…ç½®
            assert hasattr(executor, 'workflow_config')
            assert hasattr(executor, 'routing_rules')
            assert hasattr(executor, 'quality_settings')
            
            self.test_results.append({
                "test_name": test_name,
                "status": "PASS",
                "details": f"æ‰§è¡Œå™¨åˆå§‹åŒ–æ­£å¸¸ï¼Œ{len(executor.workflow_steps)}ä¸ªæ­¥éª¤ï¼Œ{len(ocr_components)}ä¸ªOCRç»„ä»¶"
            })
            
        except Exception as e:
            logger.error(f"âŒ {test_name}å¤±è´¥: {e}")
            self.test_results.append({
                "test_name": test_name,
                "status": "FAIL",
                "error": str(e)
            })
    
    async def test_request_validation(self):
        """æµ‹è¯•è¯·æ±‚éªŒè¯"""
        test_name = "è¯·æ±‚éªŒè¯æµ‹è¯•"
        logger.info(f"ğŸ§ª å¼€å§‹{test_name}")
        
        try:
            # æµ‹è¯•æœ‰æ•ˆè¯·æ±‚
            valid_request = {
                "image_path": "/tmp/test_image.jpg",
                "task_type": "document_ocr",
                "quality_level": "medium",
                "privacy_level": "normal"
            }
            
            # ç”±äºæ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¿™ä¼šåœ¨è¾“å…¥éªŒè¯æ­¥éª¤å¤±è´¥ï¼Œä½†éªŒè¯é€»è¾‘åº”è¯¥æ­£å¸¸å·¥ä½œ
            result = await self.mcp.process_ocr(valid_request)
            assert "success" in result
            assert "error" in result  # åº”è¯¥æœ‰é”™è¯¯ï¼Œå› ä¸ºæ–‡ä»¶ä¸å­˜åœ¨
            logger.info("âœ… è¯·æ±‚éªŒè¯é€»è¾‘æ­£å¸¸å·¥ä½œ")
            
            # æµ‹è¯•æ— æ•ˆè¯·æ±‚
            invalid_request = {
                "task_type": "invalid_task",
                "quality_level": "invalid_quality"
            }
            
            try:
                await self.mcp.process_ocr(invalid_request)
                assert False, "åº”è¯¥æŠ›å‡ºéªŒè¯é”™è¯¯"
            except Exception:
                logger.info("âœ… æ— æ•ˆè¯·æ±‚æ­£ç¡®è¢«æ‹’ç»")
            
            self.test_results.append({
                "test_name": test_name,
                "status": "PASS",
                "details": "è¯·æ±‚éªŒè¯é€»è¾‘æ­£å¸¸"
            })
            
        except Exception as e:
            logger.error(f"âŒ {test_name}å¤±è´¥: {e}")
            self.test_results.append({
                "test_name": test_name,
                "status": "FAIL",
                "error": str(e)
            })
    
    async def test_adapter_selection(self):
        """æµ‹è¯•é€‚é…å™¨é€‰æ‹©é€»è¾‘"""
        test_name = "é€‚é…å™¨é€‰æ‹©æµ‹è¯•"
        logger.info(f"ğŸ§ª å¼€å§‹{test_name}")
        
        try:
            executor = self.mcp.executor
            
            # æµ‹è¯•ä¸åŒæ¡ä»¶ä¸‹çš„é€‚é…å™¨é€‰æ‹©
            test_cases = [
                {
                    "task_type": "document_ocr",
                    "quality_level": "medium",
                    "privacy_level": "normal",
                    "expected": "local_model_mcp"
                },
                {
                    "task_type": "handwriting_recognition",
                    "quality_level": "high",
                    "privacy_level": "low",
                    "expected": "cloud_search_mcp"
                },
                {
                    "task_type": "document_ocr",
                    "quality_level": "medium",
                    "privacy_level": "high",
                    "expected": "local_model_mcp"  # é«˜éšç§å¼ºåˆ¶æœ¬åœ°
                }
            ]
            
            for case in test_cases:
                selected = executor._apply_routing_rules(
                    case["task_type"],
                    case["quality_level"], 
                    case["privacy_level"],
                    5.0,  # file_size_mb
                    {"quality_score": 0.7}  # image_analysis
                )
                
                logger.info(f"âœ… æ¡ä»¶ {case} -> é€‰æ‹©é€‚é…å™¨: {selected}")
                # æ³¨æ„ï¼šç”±äºè·¯ç”±é€»è¾‘å¯èƒ½æ¯”è¾ƒå¤æ‚ï¼Œè¿™é‡ŒåªéªŒè¯è¿”å›äº†æœ‰æ•ˆçš„é€‚é…å™¨
                assert selected in ["local_model_mcp", "cloud_search_mcp"]
            
            self.test_results.append({
                "test_name": test_name,
                "status": "PASS",
                "details": "é€‚é…å™¨é€‰æ‹©é€»è¾‘æ­£å¸¸"
            })
            
        except Exception as e:
            logger.error(f"âŒ {test_name}å¤±è´¥: {e}")
            self.test_results.append({
                "test_name": test_name,
                "status": "FAIL",
                "error": str(e)
            })
    
    async def test_statistics_and_monitoring(self):
        """æµ‹è¯•ç»Ÿè®¡å’Œç›‘æ§åŠŸèƒ½"""
        test_name = "ç»Ÿè®¡ç›‘æ§æµ‹è¯•"
        logger.info(f"ğŸ§ª å¼€å§‹{test_name}")
        
        try:
            # æµ‹è¯•ç»Ÿè®¡ä¿¡æ¯è·å–
            stats = self.mcp.get_statistics()
            assert "total_requests" in stats
            assert "successful_requests" in stats
            assert "failed_requests" in stats
            assert "success_rate" in stats
            logger.info(f"âœ… ç»Ÿè®¡ä¿¡æ¯è·å–æˆåŠŸ: {stats['total_requests']}ä¸ªè¯·æ±‚")
            
            # æµ‹è¯•è¯Šæ–­åŠŸèƒ½
            diagnosis = self.mcp.diagnose()
            assert "mcp_status" in diagnosis
            assert "executor_status" in diagnosis
            assert "components" in diagnosis
            assert "configuration" in diagnosis
            logger.info(f"âœ… ç³»ç»Ÿè¯Šæ–­æˆåŠŸ: MCPçŠ¶æ€ {diagnosis['mcp_status']}")
            
            self.test_results.append({
                "test_name": test_name,
                "status": "PASS",
                "details": "ç»Ÿè®¡å’Œç›‘æ§åŠŸèƒ½æ­£å¸¸"
            })
            
        except Exception as e:
            logger.error(f"âŒ {test_name}å¤±è´¥: {e}")
            self.test_results.append({
                "test_name": test_name,
                "status": "FAIL",
                "error": str(e)
            })
    
    async def test_error_handling(self):
        """æµ‹è¯•é”™è¯¯å¤„ç†"""
        test_name = "é”™è¯¯å¤„ç†æµ‹è¯•"
        logger.info(f"ğŸ§ª å¼€å§‹{test_name}")
        
        try:
            # æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨çš„é”™è¯¯å¤„ç†
            request = {
                "image_path": "/nonexistent/file.jpg",
                "task_type": "document_ocr"
            }
            
            result = await self.mcp.process_ocr(request)
            assert result["success"] == False
            assert "error" in result
            logger.info("âœ… æ–‡ä»¶ä¸å­˜åœ¨é”™è¯¯æ­£ç¡®å¤„ç†")
            
            # æµ‹è¯•æ— æ•ˆå‚æ•°çš„é”™è¯¯å¤„ç†
            invalid_request = {
                "image_path": "/tmp/test.jpg",
                "task_type": "invalid_task_type"
            }
            
            try:
                await self.mcp.process_ocr(invalid_request)
                assert False, "åº”è¯¥æŠ›å‡ºå‚æ•°éªŒè¯é”™è¯¯"
            except Exception as e:
                logger.info(f"âœ… æ— æ•ˆå‚æ•°é”™è¯¯æ­£ç¡®å¤„ç†: {e}")
            
            self.test_results.append({
                "test_name": test_name,
                "status": "PASS",
                "details": "é”™è¯¯å¤„ç†æœºåˆ¶æ­£å¸¸"
            })
            
        except Exception as e:
            logger.error(f"âŒ {test_name}å¤±è´¥: {e}")
            self.test_results.append({
                "test_name": test_name,
                "status": "FAIL",
                "error": str(e)
            })
    
    def generate_test_report(self):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        total_tests = len(self.test_results)
        passed_tests = len([r for r in self.test_results if r["status"] == "PASS"])
        failed_tests = total_tests - passed_tests
        
        report = {
            "test_summary": {
                "total_tests": total_tests,
                "passed_tests": passed_tests,
                "failed_tests": failed_tests,
                "success_rate": passed_tests / total_tests if total_tests > 0 else 0
            },
            "test_results": self.test_results,
            "overall_status": "PASS" if failed_tests == 0 else "FAIL"
        }
        
        return report
    
    async def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        logger.info("ğŸš€ å¼€å§‹OCRå·¥ä½œæµMCPé›†æˆæµ‹è¯•")
        print("=" * 80)
        print("OCRå·¥ä½œæµMCPé›†æˆæµ‹è¯•")
        print("=" * 80)
        
        # è®¾ç½®
        if not await self.setup():
            logger.error("âŒ æµ‹è¯•è®¾ç½®å¤±è´¥ï¼Œç»ˆæ­¢æµ‹è¯•")
            return
        
        # è¿è¡Œæµ‹è¯•
        test_methods = [
            self.test_basic_functionality,
            self.test_configuration_loading,
            self.test_workflow_executor,
            self.test_request_validation,
            self.test_adapter_selection,
            self.test_statistics_and_monitoring,
            self.test_error_handling
        ]
        
        for test_method in test_methods:
            try:
                await test_method()
            except Exception as e:
                logger.error(f"âŒ æµ‹è¯•æ–¹æ³• {test_method.__name__} æ‰§è¡Œå¤±è´¥: {e}")
                self.test_results.append({
                    "test_name": test_method.__name__,
                    "status": "FAIL",
                    "error": f"æµ‹è¯•æ‰§è¡Œå¼‚å¸¸: {e}"
                })
        
        # ç”ŸæˆæŠ¥å‘Š
        report = self.generate_test_report()
        
        # æ‰“å°ç»“æœ
        print("\n" + "=" * 80)
        print("æµ‹è¯•ç»“æœæ±‡æ€»")
        print("=" * 80)
        print(f"æ€»æµ‹è¯•æ•°: {report['test_summary']['total_tests']}")
        print(f"é€šè¿‡æµ‹è¯•: {report['test_summary']['passed_tests']}")
        print(f"å¤±è´¥æµ‹è¯•: {report['test_summary']['failed_tests']}")
        print(f"æˆåŠŸç‡: {report['test_summary']['success_rate']:.2%}")
        print(f"æ•´ä½“çŠ¶æ€: {report['overall_status']}")
        
        print("\nè¯¦ç»†ç»“æœ:")
        for result in report['test_results']:
            status_icon = "âœ…" if result['status'] == "PASS" else "âŒ"
            print(f"{status_icon} {result['test_name']}: {result['status']}")
            if result['status'] == "PASS" and 'details' in result:
                print(f"   è¯¦æƒ…: {result['details']}")
            elif result['status'] == "FAIL" and 'error' in result:
                print(f"   é”™è¯¯: {result['error']}")
        
        # ä¿å­˜æŠ¥å‘Š
        report_path = Path(__file__).parent / "test_report.json"
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ğŸ“Š æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")
        
        return report

async def main():
    """ä¸»å‡½æ•°"""
    tester = OCRWorkflowIntegrationTest()
    report = await tester.run_all_tests()
    
    # æ ¹æ®æµ‹è¯•ç»“æœè®¾ç½®é€€å‡ºç 
    exit_code = 0 if report['overall_status'] == "PASS" else 1
    sys.exit(exit_code)

if __name__ == "__main__":
    asyncio.run(main())

