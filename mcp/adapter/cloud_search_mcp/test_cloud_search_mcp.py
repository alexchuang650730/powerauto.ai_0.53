#!/usr/bin/env python3
"""
Cloud Search MCP æµ‹è¯•å¥—ä»¶

å…¨é¢æµ‹è¯•Cloud Search MCPçš„å„é¡¹åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
- å¤šæ¨¡å‹OCRæµ‹è¯•
- æ™ºèƒ½è·¯ç”±æµ‹è¯•
- é™çº§æœºåˆ¶æµ‹è¯•
- æ€§èƒ½åŸºå‡†æµ‹è¯•
"""

import asyncio
import json
import time
import base64
from pathlib import Path
from typing import Dict, Any, List
import sys

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent))

from cloud_search_mcp import CloudSearchMCP, TaskType

class CloudSearchMCPTester:
    """Cloud Search MCPæµ‹è¯•å™¨"""
    
    def __init__(self, config_path: str = "config.toml"):
        self.config_path = config_path
        self.mcp = None
        self.test_results = []
    
    async def setup(self):
        """åˆå§‹åŒ–æµ‹è¯•ç¯å¢ƒ"""
        print("ğŸš€ åˆå§‹åŒ–Cloud Search MCPæµ‹è¯•ç¯å¢ƒ...")
        self.mcp = CloudSearchMCP(self.config_path)
        print(f"âœ… MCPåˆå§‹åŒ–å®Œæˆï¼Œå¯ç”¨æ¨¡å‹: {list(self.mcp.model_configs.keys())}")
    
    async def test_basic_functionality(self):
        """æµ‹è¯•åŸºæœ¬åŠŸèƒ½"""
        print("\n" + "="*60)
        print("ğŸ§ª æµ‹è¯•åŸºæœ¬åŠŸèƒ½")
        print("="*60)
        
        # æµ‹è¯•å¥åº·æ£€æŸ¥
        result = self.mcp.health_check()
        assert result["status"] == "success", "å¥åº·æ£€æŸ¥å¤±è´¥"
        print("âœ… å¥åº·æ£€æŸ¥é€šè¿‡")
        
        # æµ‹è¯•è·å–èƒ½åŠ›
        result = self.mcp.get_capabilities()
        assert result["status"] == "success", "è·å–èƒ½åŠ›å¤±è´¥"
        assert len(result["capabilities"]) > 0, "èƒ½åŠ›åˆ—è¡¨ä¸ºç©º"
        print(f"âœ… èƒ½åŠ›æ£€æŸ¥é€šè¿‡ï¼Œæ”¯æŒ {len(result['capabilities'])} ç§ä»»åŠ¡ç±»å‹")
        
        # æµ‹è¯•è·å–æ¨¡å‹åˆ—è¡¨
        result = self.mcp.get_supported_models()
        assert result["status"] == "success", "è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥"
        assert len(result["models"]) > 0, "æ¨¡å‹åˆ—è¡¨ä¸ºç©º"
        print(f"âœ… æ¨¡å‹æ£€æŸ¥é€šè¿‡ï¼Œæ”¯æŒ {len(result['models'])} ä¸ªæ¨¡å‹")
        
        self.test_results.append({
            "test": "basic_functionality",
            "status": "passed",
            "details": "æ‰€æœ‰åŸºæœ¬åŠŸèƒ½æµ‹è¯•é€šè¿‡"
        })
    
    async def test_ocr_with_sample_image(self):
        """ä½¿ç”¨ç¤ºä¾‹å›¾åƒæµ‹è¯•OCR"""
        print("\n" + "="*60)
        print("ğŸ–¼ï¸ æµ‹è¯•OCRåŠŸèƒ½ï¼ˆç¤ºä¾‹å›¾åƒï¼‰")
        print("="*60)
        
        # åˆ›å»ºä¸€ä¸ªç®€å•çš„æµ‹è¯•å›¾åƒï¼ˆ1x1åƒç´ PNGï¼‰
        test_image_data = self._create_test_image()
        
        # æµ‹è¯•ä¸åŒä»»åŠ¡ç±»å‹
        task_types = [
            TaskType.DOCUMENT_OCR,
            TaskType.HANDWRITING_OCR,
            TaskType.TABLE_EXTRACTION
        ]
        
        for task_type in task_types:
            print(f"\nğŸ” æµ‹è¯•ä»»åŠ¡ç±»å‹: {task_type.value}")
            
            try:
                result = await self.mcp.process_ocr_request(
                    image_data=test_image_data,
                    task_type=task_type.value,
                    language="auto",
                    output_format="markdown"
                )
                
                if result["status"] == "success":
                    response = result["result"]
                    print(f"âœ… {task_type.value} æµ‹è¯•æˆåŠŸ")
                    print(f"   æ¨¡å‹: {response['model_used']}")
                    print(f"   ç½®ä¿¡åº¦: {response['confidence']:.2%}")
                    print(f"   å¤„ç†æ—¶é—´: {response['processing_time']:.2f}ç§’")
                    print(f"   æˆæœ¬: ${response['cost']:.6f}")
                else:
                    print(f"âŒ {task_type.value} æµ‹è¯•å¤±è´¥: {result.get('message')}")
                
                # çŸ­æš‚å»¶è¿Ÿé¿å…APIé™åˆ¶
                await asyncio.sleep(2)
                
            except Exception as e:
                print(f"âŒ {task_type.value} æµ‹è¯•å¼‚å¸¸: {e}")
        
        self.test_results.append({
            "test": "ocr_sample_image",
            "status": "completed",
            "details": f"æµ‹è¯•äº† {len(task_types)} ç§ä»»åŠ¡ç±»å‹"
        })
    
    def _create_test_image(self) -> bytes:
        """åˆ›å»ºæµ‹è¯•å›¾åƒï¼ˆ1x1åƒç´ PNGï¼‰"""
        # æœ€å°çš„PNGå›¾åƒæ•°æ®ï¼ˆ1x1åƒç´ ï¼Œé€æ˜ï¼‰
        png_data = base64.b64decode(
            "iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVR42mNkYPhfDwAChAI9jU77zgAAAABJRU5ErkJggg=="
        )
        return png_data
    
    async def test_model_selection(self):
        """æµ‹è¯•æ¨¡å‹é€‰æ‹©é€»è¾‘"""
        print("\n" + "="*60)
        print("ğŸ¤– æµ‹è¯•æ¨¡å‹é€‰æ‹©é€»è¾‘")
        print("="*60)
        
        # æµ‹è¯•ä¸åŒä¼˜å…ˆçº§çš„æ¨¡å‹é€‰æ‹©
        priorities = ["speed", "cost", "quality", "balanced"]
        task_type = TaskType.DOCUMENT_OCR
        
        for priority in priorities:
            # ä¸´æ—¶ä¿®æ”¹é…ç½®
            original_priority = self.mcp.config.get("cloud_search_mcp", {}).get("priority")
            self.mcp.config.setdefault("cloud_search_mcp", {})["priority"] = priority
            
            selected_model = self.mcp.model_selector.select_optimal_model(task_type, priority)
            
            if selected_model:
                print(f"âœ… {priority} ä¼˜å…ˆçº§é€‰æ‹©æ¨¡å‹: {selected_model.value}")
            else:
                print(f"âŒ {priority} ä¼˜å…ˆçº§æœªé€‰æ‹©åˆ°æ¨¡å‹")
            
            # æ¢å¤åŸé…ç½®
            if original_priority:
                self.mcp.config["cloud_search_mcp"]["priority"] = original_priority
        
        self.test_results.append({
            "test": "model_selection",
            "status": "completed",
            "details": f"æµ‹è¯•äº† {len(priorities)} ç§ä¼˜å…ˆçº§ç­–ç•¥"
        })
    
    async def test_error_handling(self):
        """æµ‹è¯•é”™è¯¯å¤„ç†"""
        print("\n" + "="*60)
        print("ğŸš¨ æµ‹è¯•é”™è¯¯å¤„ç†")
        print("="*60)
        
        # æµ‹è¯•æ— æ•ˆå›¾åƒæ•°æ®
        try:
            result = await self.mcp.process_ocr_request(
                image_data=b"invalid_image_data",
                task_type="document_ocr"
            )
            
            if result["status"] == "error":
                print("âœ… æ— æ•ˆå›¾åƒæ•°æ®é”™è¯¯å¤„ç†æ­£ç¡®")
            else:
                print("âŒ æ— æ•ˆå›¾åƒæ•°æ®åº”è¯¥è¿”å›é”™è¯¯")
                
        except Exception as e:
            print(f"âœ… æ— æ•ˆå›¾åƒæ•°æ®è§¦å‘å¼‚å¸¸ï¼ˆé¢„æœŸè¡Œä¸ºï¼‰: {e}")
        
        # æµ‹è¯•æ— æ•ˆä»»åŠ¡ç±»å‹
        try:
            result = await self.mcp.process_ocr_request(
                image_data=self._create_test_image(),
                task_type="invalid_task_type"
            )
            print("âœ… æ— æ•ˆä»»åŠ¡ç±»å‹å¤„ç†å®Œæˆ")
            
        except Exception as e:
            print(f"âœ… æ— æ•ˆä»»åŠ¡ç±»å‹è§¦å‘å¼‚å¸¸ï¼ˆé¢„æœŸè¡Œä¸ºï¼‰: {e}")
        
        # æµ‹è¯•ç¼ºå°‘å‚æ•°
        try:
            result = await self.mcp.process_ocr_request()
            if result["status"] == "error":
                print("âœ… ç¼ºå°‘å‚æ•°é”™è¯¯å¤„ç†æ­£ç¡®")
            
        except Exception as e:
            print(f"âœ… ç¼ºå°‘å‚æ•°è§¦å‘å¼‚å¸¸ï¼ˆé¢„æœŸè¡Œä¸ºï¼‰: {e}")
        
        self.test_results.append({
            "test": "error_handling",
            "status": "passed",
            "details": "é”™è¯¯å¤„ç†æœºåˆ¶å·¥ä½œæ­£å¸¸"
        })
    
    async def test_mcp_interface(self):
        """æµ‹è¯•MCPæ ‡å‡†æ¥å£"""
        print("\n" + "="*60)
        print("ğŸ”Œ æµ‹è¯•MCPæ ‡å‡†æ¥å£")
        print("="*60)
        
        # æµ‹è¯•processæ–¹æ³•
        test_cases = [
            {
                "operation": "get_capabilities",
                "params": {}
            },
            {
                "operation": "get_supported_models", 
                "params": {}
            },
            {
                "operation": "health_check",
                "params": {}
            },
            {
                "operation": "get_statistics",
                "params": {}
            }
        ]
        
        for test_case in test_cases:
            operation = test_case["operation"]
            result = self.mcp.process(test_case)
            
            if result["status"] == "success":
                print(f"âœ… MCPæ¥å£ {operation} æµ‹è¯•é€šè¿‡")
            else:
                print(f"âŒ MCPæ¥å£ {operation} æµ‹è¯•å¤±è´¥: {result.get('message')}")
        
        # æµ‹è¯•æ— æ•ˆæ“ä½œ
        result = self.mcp.process({
            "operation": "invalid_operation",
            "params": {}
        })
        
        if result["status"] == "error":
            print("âœ… æ— æ•ˆæ“ä½œé”™è¯¯å¤„ç†æ­£ç¡®")
        else:
            print("âŒ æ— æ•ˆæ“ä½œåº”è¯¥è¿”å›é”™è¯¯")
        
        self.test_results.append({
            "test": "mcp_interface",
            "status": "passed",
            "details": "MCPæ ‡å‡†æ¥å£æµ‹è¯•é€šè¿‡"
        })
    
    async def test_performance_benchmark(self):
        """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        print("\n" + "="*60)
        print("âš¡ æ€§èƒ½åŸºå‡†æµ‹è¯•")
        print("="*60)
        
        test_image = self._create_test_image()
        num_requests = 5
        
        print(f"ğŸ”„ æ‰§è¡Œ {num_requests} æ¬¡OCRè¯·æ±‚...")
        
        start_time = time.time()
        successful_requests = 0
        total_cost = 0.0
        processing_times = []
        
        for i in range(num_requests):
            try:
                result = await self.mcp.process_ocr_request(
                    image_data=test_image,
                    task_type="document_ocr",
                    language="auto"
                )
                
                if result["status"] == "success":
                    successful_requests += 1
                    response = result["result"]
                    total_cost += response["cost"]
                    processing_times.append(response["processing_time"])
                
                print(f"  è¯·æ±‚ {i+1}/{num_requests} å®Œæˆ")
                
                # é¿å…APIé™åˆ¶
                if i < num_requests - 1:
                    await asyncio.sleep(1)
                    
            except Exception as e:
                print(f"  è¯·æ±‚ {i+1} å¤±è´¥: {e}")
        
        total_time = time.time() - start_time
        
        print(f"\nğŸ“Š æ€§èƒ½ç»Ÿè®¡:")
        print(f"   æ€»è¯·æ±‚æ•°: {num_requests}")
        print(f"   æˆåŠŸè¯·æ±‚æ•°: {successful_requests}")
        print(f"   æˆåŠŸç‡: {successful_requests/num_requests*100:.1f}%")
        print(f"   æ€»è€—æ—¶: {total_time:.2f}ç§’")
        print(f"   å¹³å‡æ¯è¯·æ±‚: {total_time/num_requests:.2f}ç§’")
        print(f"   æ€»æˆæœ¬: ${total_cost:.6f}")
        print(f"   å¹³å‡æˆæœ¬: ${total_cost/max(1, successful_requests):.6f}")
        
        if processing_times:
            avg_processing = sum(processing_times) / len(processing_times)
            print(f"   å¹³å‡å¤„ç†æ—¶é—´: {avg_processing:.2f}ç§’")
        
        self.test_results.append({
            "test": "performance_benchmark",
            "status": "completed",
            "details": {
                "total_requests": num_requests,
                "successful_requests": successful_requests,
                "success_rate": successful_requests/num_requests*100,
                "total_time": total_time,
                "total_cost": total_cost
            }
        })
    
    async def test_statistics_tracking(self):
        """æµ‹è¯•ç»Ÿè®¡ä¿¡æ¯è·Ÿè¸ª"""
        print("\n" + "="*60)
        print("ğŸ“ˆ æµ‹è¯•ç»Ÿè®¡ä¿¡æ¯è·Ÿè¸ª")
        print("="*60)
        
        # è·å–åˆå§‹ç»Ÿè®¡
        initial_stats = self.mcp.get_statistics()["statistics"]
        print(f"ğŸ“Š åˆå§‹ç»Ÿè®¡: {initial_stats['total_requests']} ä¸ªè¯·æ±‚")
        
        # æ‰§è¡Œä¸€äº›è¯·æ±‚
        test_image = self._create_test_image()
        
        for i in range(3):
            await self.mcp.process_ocr_request(
                image_data=test_image,
                task_type="document_ocr"
            )
            await asyncio.sleep(1)
        
        # è·å–æ›´æ–°åçš„ç»Ÿè®¡
        final_stats = self.mcp.get_statistics()["statistics"]
        print(f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡: {final_stats['total_requests']} ä¸ªè¯·æ±‚")
        
        # éªŒè¯ç»Ÿè®¡æ›´æ–°
        requests_diff = final_stats['total_requests'] - initial_stats['total_requests']
        if requests_diff >= 3:
            print("âœ… ç»Ÿè®¡ä¿¡æ¯è·Ÿè¸ªæ­£å¸¸")
        else:
            print(f"âŒ ç»Ÿè®¡ä¿¡æ¯è·Ÿè¸ªå¼‚å¸¸ï¼Œé¢„æœŸå¢åŠ 3ä¸ªè¯·æ±‚ï¼Œå®é™…å¢åŠ {requests_diff}ä¸ª")
        
        # æ˜¾ç¤ºè¯¦ç»†ç»Ÿè®¡
        print(f"ğŸ“ˆ è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯:")
        for key, value in final_stats.items():
            if isinstance(value, dict):
                print(f"   {key}:")
                for sub_key, sub_value in value.items():
                    print(f"     {sub_key}: {sub_value}")
            else:
                print(f"   {key}: {value}")
        
        self.test_results.append({
            "test": "statistics_tracking",
            "status": "passed",
            "details": "ç»Ÿè®¡ä¿¡æ¯è·Ÿè¸ªåŠŸèƒ½æ­£å¸¸"
        })
    
    def generate_test_report(self):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        print("\n" + "="*60)
        print("ğŸ“‹ æµ‹è¯•æŠ¥å‘Š")
        print("="*60)
        
        total_tests = len(self.test_results)
        passed_tests = len([r for r in self.test_results if r["status"] == "passed"])
        completed_tests = len([r for r in self.test_results if r["status"] in ["passed", "completed"]])
        
        print(f"ğŸ“Š æµ‹è¯•æ¦‚è§ˆ:")
        print(f"   æ€»æµ‹è¯•æ•°: {total_tests}")
        print(f"   é€šè¿‡æµ‹è¯•: {passed_tests}")
        print(f"   å®Œæˆæµ‹è¯•: {completed_tests}")
        print(f"   æˆåŠŸç‡: {completed_tests/total_tests*100:.1f}%")
        
        print(f"\nğŸ“ è¯¦ç»†ç»“æœ:")
        for result in self.test_results:
            status_icon = "âœ…" if result["status"] in ["passed", "completed"] else "âŒ"
            print(f"   {status_icon} {result['test']}: {result['details']}")
        
        # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
        report_data = {
            "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
            "summary": {
                "total_tests": total_tests,
                "passed_tests": passed_tests,
                "completed_tests": completed_tests,
                "success_rate": completed_tests/total_tests*100
            },
            "results": self.test_results
        }
        
        report_file = Path("test_report.json")
        with open(report_file, "w", encoding="utf-8") as f:
            json.dump(report_data, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ’¾ æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
    
    async def run_all_tests(self):
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        print("ğŸ§ª Cloud Search MCP å…¨é¢æµ‹è¯•å¼€å§‹")
        print("="*60)
        
        try:
            await self.setup()
            await self.test_basic_functionality()
            await self.test_ocr_with_sample_image()
            await self.test_model_selection()
            await self.test_error_handling()
            await self.test_mcp_interface()
            await self.test_performance_benchmark()
            await self.test_statistics_tracking()
            
        except Exception as e:
            print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()
            
            self.test_results.append({
                "test": "test_execution",
                "status": "failed",
                "details": f"æµ‹è¯•æ‰§è¡Œå¼‚å¸¸: {str(e)}"
            })
        
        finally:
            self.generate_test_report()

async def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Cloud Search MCP æµ‹è¯•å¥—ä»¶")
    parser.add_argument("--config", default="config.toml", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--test", choices=[
        "basic", "ocr", "models", "errors", "mcp", "performance", "stats", "all"
    ], default="all", help="è¦è¿è¡Œçš„æµ‹è¯•ç±»å‹")
    
    args = parser.parse_args()
    
    tester = CloudSearchMCPTester(args.config)
    
    if args.test == "all":
        await tester.run_all_tests()
    else:
        await tester.setup()
        
        if args.test == "basic":
            await tester.test_basic_functionality()
        elif args.test == "ocr":
            await tester.test_ocr_with_sample_image()
        elif args.test == "models":
            await tester.test_model_selection()
        elif args.test == "errors":
            await tester.test_error_handling()
        elif args.test == "mcp":
            await tester.test_mcp_interface()
        elif args.test == "performance":
            await tester.test_performance_benchmark()
        elif args.test == "stats":
            await tester.test_statistics_tracking()
        
        tester.generate_test_report()

if __name__ == "__main__":
    asyncio.run(main())

